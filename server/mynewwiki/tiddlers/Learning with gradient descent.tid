created: 20190708021813658
modified: 20190714011258158
tags: draft neural-networks
title: Learning with gradient descent
type: text/vnd.tiddlywiki

! Gradient Descent

<<<
We want to find a set of weights and biases which make the [[cost|Cost function]] as small as possible. We'll do that using an algorithm known as ''gradient descent''.
<<<

TODO: elaborate on

$$
\Delta C \approx \nabla C . \Delta v
$$

TODO: elaborate on update rules

$$
w_{k} \rightarrow w_{k}^{\prime} = w_{k} - \eta \frac{\partial C}{\partial w_{k}}
$$

$$
b_{l} \rightarrow b_{l}^{\prime} = b_{l} - \eta \frac{\partial C}{\partial b_{l}}
$$

TODO: stochastic gradient descent 

$$
\frac{\sum_{j=1}^{m} \nabla C_{X_{j}}}{m} \approx \frac{\sum_{x} \nabla C_{x} }{x} = \nabla C
$$

TODO: epoch of training